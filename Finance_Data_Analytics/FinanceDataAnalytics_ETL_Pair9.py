# -*- coding: utf-8 -*-
"""FinanceDataAnalytics_ETL_Pair9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KtBZLONs-4znEn5CLXzi29qh5BEhisdf
"""

# Importing necessary modules
from airflow import DAG
from airflow.models import Variable
from airflow.models import DagRun
from airflow.decorators import task
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
import snowflake.connector
import requests
from datetime import datetime, timedelta
from airflow.operators.dagrun_operator import TriggerDagRunOperator

def return_snowflake_conn():
    hook = SnowflakeHook(snowflake_conn_id='snowflake_conn') # Initialize the SnowflakeHook
    conn = hook.get_conn()
    return conn.cursor() # Created a cursor object to work with databases


#Function to initialize target table
@task
def initialize_target_table(table):
    cursor = (return_snowflake_conn())
    # Define Query for Table Create or Replace by using table parameter passed to this function
    table_create = f"""
    CREATE OR REPLACE TABLE {table} (
        date TIMESTAMP_NTZ NOT NULL,
        open FLOAT NOT NULL,
        high FLOAT NOT NULL,
        low FLOAT NOT NULL,
        close FLOAT NOT NULL,
        volume INT NOT NULL,
        symbol STRING NOT NULL,
        PRIMARY KEY (date, symbol)  -- Define composite primary key
    );
    """
    cursor.execute(table_create)  # Create a new table with required fields if not existing, else replace
    print("Target Table Initialized and Ready to Store Data from ETL")


# Creating Function to read stock prices based on symbol provided
@task
def extract_stock_data(symbol):
    if(symbol == "AAPL"):
        print("Starting Extraction for Apple Stock Data...")
    elif(symbol == "NVDA"):
        print("Starting Extraction for NVIDIA Stock Data...")
    vantage_api_key = Variable.get('vantage_api_key')
    url = f"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={vantage_api_key}"
    r = requests.get(url)
    data = (r.json())  # Storing all data for given symbol in json form, fetched from alpha vantage api
    print("<---------- Last Refreshed:",data['Meta Data']['3. Last Refreshed'],"\n\nSymbol:",data['Meta Data']['2. Symbol'],"---------->")
    results = []  # Initializing empty list
    for d in data["Time Series (Daily)"]:  # Here the keys under 'Time Series (Daily)' are stored, which are the dates
        stock_info = data["Time Series (Daily)"][d]  # This stores the stock prices info for each iterated date
        stock_info["date"] = d  # Creating another key value pair for date to the end of stock_info dictionary
        results.append(stock_info)  # Each stock_info instance of dictionary that holds the stock prices along with that particular date is added to results list
    print("Stock Data Extraction Complete\n")
    return results


# Transforming stock data to return only 90d data
@task
def transform_to_90d_stock_data(results):
    print("Transforming the Extracted Stock Data...")
    today = datetime.now().date()  # Get today's date
    ninety_days_ago = today - timedelta(days=90)  # Calculate the date 90 days ago

    # Filter results for the last 90 days
    filtered_results = [
        entry
        for entry in results
        if datetime.strptime(entry["date"], "%Y-%m-%d").date() >= ninety_days_ago
    ]
    print("Completed Transforming Stock Data\n")
    return filtered_results


# Loading last 90d data into snowflake table
@task
def load_stock_data(table, results, symbol):
    try:
        cursor = (return_snowflake_conn())  # Created cursor object to execute snowflake operations
        cursor.execute("BEGIN;")  # Starting a transaction
        print(f"{symbol} Stock Data Ready to Load on Snowflake. Beginning load...")  # Acknowledging

        index = 1  # Initialize variable to keep count of inserts executed

        for (r) in (results):  # Loading records to snowflake table through insert query for each record from results
            open = r["1. open"]
            high = r["2. high"]
            low = r["3. low"]
            close = r["4. close"]
            volume = r["5. volume"]
            date = r["date"]

            # Using MERGE to handle existing records
            merge_sql = f"""
            MERGE INTO {table} AS target
            USING (SELECT '{date}' AS date, '{open}' AS open, '{high}' AS high, '{low}' AS low, '{close}' AS close, '{volume}' AS volume, '{symbol}' AS symbol) AS source
            ON target.symbol = source.symbol and target.date = source.date
            WHEN MATCHED THEN
                UPDATE SET
                    date = source.date,
                    open = source.open,
                    high = source.high,
                    low = source.low,
                    close = source.close,
                    volume = source.volume
            WHEN NOT MATCHED THEN
                INSERT (date, open, high, low, close, volume, symbol)
                VALUES (source.date, source.open, source.high, source.low, source.close, source.volume, source.symbol);
            """
            print("Executing: ", merge_sql)
            cursor.execute(merge_sql)
            print("Number of Inserts Executed: ", index)
            index += 1
        cursor.execute("COMMIT;")
        print("Number of Inserts that were expected to Execute: ", len(results)) # Displaying length of results to be matched against iterated index
        print(f"Load complete for {symbol} Stock Data")
    except Exception as e:  # Roll back entire operation in case of errors during load
        cursor.execute("ROLLBACK;")
        print(f"An error occurred: {e}")


def trigger_prediction_dag(context):
    dag_run = context['dag_run']
    dag_id = 'Stock_Prediction'

    # Triggering the Stock_Prediction DAG
    dag_run = context['dag'].bag.dagbag.get_dag(dag_id)
    dag_run.create_dagrun(
        run_id='triggered_run_' + dag_run.run_id,
        execution_date=datetime.now(),
        state='running',
        conf={},
        external_trigger=True,
    )
    print("Triggered Stock Prediction DAG")


#Setting up DAG with Parameters
with DAG(
    dag_id="Stock_Data_ETL",
    default_args={
        "owner": "Pair 9",
        "email": ["shatayu.thakur@sjsu.edu", "pranav.raveendran@sjsu.edu"],
        "email_on_failure": True,
        "email_on_retry": True,
        "email_on_success": True,
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    },
    on_success_callback=trigger_prediction_dag,
    start_date=datetime(2024, 10, 12),
    catchup=False,
    tags=["ETL"],
    schedule_interval='30 21 * * *',  # This will run daily at 2:30 PM PT i.e. API Refresh Time
) as dag:

    #Setting Variables for ETL
    target_table = "dev.raw_data.stock_data"
    symbol_apple = "AAPL"
    symbol_nvidia = "NVDA"

    # Initializing the target table before starting ETL
    initialize_task = initialize_target_table(target_table)

    # Apple ETL Sequence
    apple_data = extract_stock_data(symbol_apple) # Extracting stock price data for Apple
    transformed_apple = transform_to_90d_stock_data(apple_data) # Transforming stock price records to include data from last 90d
    load_apple = load_stock_data(target_table, transformed_apple, symbol_apple) # Loading the extracted Apple stock price data
    print("-----ETL Completed for Apple Stock Data-----\n\n")


    # NVIDIA ETL Sequence
    nvidia_data = extract_stock_data(symbol_nvidia)  # Extracting stock price data for Nvidia
    transformed_nvidia = transform_to_90d_stock_data(nvidia_data)  # Transforming stock price records to include data from last 90d
    load_nvidia = load_stock_data(target_table, transformed_nvidia, symbol_nvidia)  # Loading the extracted Nvidia stock price data
    print("-----ETL Completed for NVIDIA Stock Data-----\n\n")

    # Task dependencies
    initialize_task >> [load_apple, load_nvidia]

     # Triggering the Stock_Prediction DAG on success
    trigger_prediction_task = TriggerDagRunOperator(
        task_id='trigger_stock_prediction',
        trigger_dag_id='Stock_Prediction',
        conf={},  # Pass any configuration needed by the triggered DAG
        wait_for_completion=True,  # Optionally wait for the triggered DAG to complete
    )

    # Setting the dependency for triggering the prediction DAG after loading the stock data
    [load_apple, load_nvidia] >> trigger_prediction_task
